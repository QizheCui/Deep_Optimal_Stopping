{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DOS import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 0.05  #interest rate\n",
    "dividend = 0.1  #divident rate\n",
    "seed_train = 1 # seed for training data\n",
    "seed_test = 2 # seed for reproducibility\n",
    "learning_rate = 0.001 #learning rate\n",
    "k = 100.0  #strike price\n",
    "test_paths = 100000 # number of paths for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "d = 5   #dimension of GBM\n",
    "mu = (r - dividend) * np.ones(shape = (d, )) # drift\n",
    "sigma = 0.2 * np.ones(shape = (d, ))\n",
    "S0 = 100.0 * np.ones(shape = (d, )) #initial price\n",
    "# we use lessp paths due to limited computational resources\n",
    "base_steps = 1500 #number of training steps regardless of the dimension\n",
    "total_paths = batch_size * (base_steps + d) #total number of paths, base_steps+d is the training steps, where each step we need 8192 paths\n",
    "number_of_training_steps = int(total_paths / batch_size) #number of training steps\n",
    "\n",
    "#Simulating GBM paths\n",
    "X = GBM(d, mu, sigma, S0, T, dt, total_paths, seed=seed_train) #simulating GBM paths\n",
    "X = X.reshape(base_steps+d, batch_size, d, 10) #reshaping to (3000+d, 8192, d, 10) to simulate 3000+d many paths of 8192 samples of 5 dimensional GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X \\in R^{d}$ be the input, then the network is parametrized as:\n",
    "\\begin{equation}\n",
    "F^{\\theta} = \\sigma \\circ a_4^{\\theta} \\circ \\phi_3 \\circ \\beta_3 \\circ \\gamma_3 \\circ a_3^{\\theta} \\circ \\phi_2 \\circ \\beta_2 \\circ \\gamma_2 \\circ a_2^{\\theta} \\circ \\phi_1 \\circ \\beta_1 \\circ \\gamma_1 \\circ a_1^{\\theta}\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "- $a_1^{\\theta} : \\mathbb{R}^d \\to \\mathbb{R}^{d+50}, a_2^{\\theta} : \\mathbb{R}^{d+50} \\to \\mathbb{R}^{d+50}, a_3^{\\theta} : \\mathbb{R}^{d+50} \\to \\mathbb{R}^{d+50}, a_4^{\\theta} : \\mathbb{R}^{d+50} \\to \\mathbb{R}$ are linear functions of the form $a_i^{\\theta}(x) = W_ix + b_i$, with $W_1 \\in \\mathbb{R}^{(d+50) \\times d}, W_2, W_3 \\in \\mathbb{R}^{(d+50) \\times (d+50)}, W_4 \\in \\mathbb{R}^{1 \\times (d+50)}$ as weight matrices and $b_1, b_2, b_3 \\in \\mathbb{R}^{d+50}, b_4 \\in \\mathbb{R}$ as bias vectors. \n",
    "\n",
    "- $\\phi_i : \\mathbb{R}^{d+50} \\to \\mathbb{R}^{d+50}$ for $i=1,2,3$ is the ReLU activation function, which just takes the maximum of each component and 0, meaning $\\phi_i(x_1, \\ldots, x_{d+50}) = (x_1^+, \\ldots, x_{d+50}^+)$.\n",
    "\n",
    "- $\\beta_i : \\mathbb{R}^{d+50} \\to \\mathbb{R}^{d+50}$ for $i=1,2,3$ is the batch normalization function.\n",
    "\n",
    "- $\\gamma_i : \\mathbb{R}^{d+50} \\to \\mathbb{R}^{d+50}$ for $i=1,2,3$ is a dropout layer function with a dropout rate of 0.5.\n",
    "\n",
    "- $\\sigma : \\mathbb{R} \\to (0,1)$ is the logistic sigmoid function, $\\sigma(x) = 1/(1 + e^{-x})$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the model for 8th stopping time has been trained\n",
      "the model for 7th stopping time has been trained\n",
      "the model for 6th stopping time has been trained\n",
      "the model for 5th stopping time has been trained\n",
      "the model for 4th stopping time has been trained\n",
      "the model for 3th stopping time has been trained\n",
      "the model for 2th stopping time has been trained\n",
      "the model for 1th stopping time has been trained\n"
     ]
    }
   ],
   "source": [
    "# training the model for each stopping time\n",
    "for i in range(N-1, 0, -1):\n",
    "    model = create_randomized_model(d).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    train(X, r, k, dt, model, i, optimizer, number_of_training_steps, batch_size)\n",
    "    # store the model and the stopping decision function and stopping time function\n",
    "    F[i] = model\n",
    "    f[i] = lambda x, i=i: fi(x, i, F) #  store the stopping decision function\n",
    "    l[i] = lambda x, i=i: li(x, i, f, l) # store the stopping time function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated price of the 5D Bermudan Max Call Option: 24.16\n"
     ]
    }
   ],
   "source": [
    "# test the model on another set of paths\n",
    "X = GBM(d, mu, sigma, S0, T, dt, test_paths, seed = seed_test)\n",
    "g_val = g(X, r, k, dt) # g values at stopping times\n",
    "X = torch.from_numpy(X).float().to(device) # convert X to a tensor\n",
    "Z = g_val[range(test_paths), l[1](X)] # g values at stopping times, l[1] \n",
    "price = 1 / test_paths * np.sum(Z) # monte carlo estimate of the price\n",
    "print(f\"Estimated price of the {d}D Bermudan Max Call Option: {price:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
